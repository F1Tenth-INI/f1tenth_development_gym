
Number of samples in training set: 278535
The mean number of samples from each experiment used for training is 2995.0 with variance 0.0
Number of samples in validation set: 38935

2433/2433 [==============================] - 3s 1ms/step - loss: 0.0065

Validation loss before starting training is 0.006474281661212444
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 prune_low_magnitude_layers  (None, 1, 64)             8130      
 _0 (PruneLowMagnitude)                                          
                                                                 
 prune_low_magnitude_activa  (None, 1, 64)             1         
 tion (PruneLowMagnitude)                                        
                                                                 
 prune_low_magnitude_layers  (None, 1, 64)             8258      
 _1 (PruneLowMagnitude)                                          
                                                                 
 prune_low_magnitude_activa  (None, 1, 64)             1         
 tion_1 (PruneLowMagnitude)                                      
                                                                 
 prune_low_magnitude_layers  (None, 1, 2)              260       
 _2 (PruneLowMagnitude)                                          
                                                                 
=================================================================
Total params: 16650 (65.06 KB)
Trainable params: 8386 (32.76 KB)
Non-trainable params: 8264 (32.30 KB)
_________________________________________________________________
Epoch 1/15
17408/17408 [==============================] - 57s 3ms/step - loss: 0.0060 - val_loss: 0.0066 - lr: 0.0010

Epoch 2/15
17408/17408 [==============================] - 54s 3ms/step - loss: 0.0046 - val_loss: 0.0036 - lr: 0.0010

Epoch 3/15
17408/17408 [==============================] - 55s 3ms/step - loss: 0.0027 - val_loss: 0.0026 - lr: 0.0010

Epoch 4/15
17408/17408 [==============================] - 56s 3ms/step - loss: 0.0022 - val_loss: 0.0021 - lr: 0.0010

Epoch 5/15
17387/17408 [============================>.] - ETA: 0s - loss: 0.0019

Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
17408/17408 [==============================] - 53s 3ms/step - loss: 0.0019 - val_loss: 0.0020 - lr: 0.0010

Epoch 6/15
17408/17408 [==============================] - 55s 3ms/step - loss: 0.0017 - val_loss: 0.0018 - lr: 2.0000e-04

Epoch 7/15
17400/17408 [============================>.] - ETA: 0s - loss: 0.0016

Epoch 7: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
17408/17408 [==============================] - 55s 3ms/step - loss: 0.0016 - val_loss: 0.0017 - lr: 2.0000e-04

Epoch 8/15
17391/17408 [============================>.] - ETA: 0s - loss: 0.0016

Epoch 8: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
17408/17408 [==============================] - 58s 3ms/step - loss: 0.0016 - val_loss: 0.0017 - lr: 4.0000e-05

Epoch 9/15
17395/17408 [============================>.] - ETA: 0s - loss: 0.0015

Epoch 9: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
17408/17408 [==============================] - 56s 3ms/step - loss: 0.0015 - val_loss: 0.0017 - lr: 8.0000e-06

Epoch 10/15
17388/17408 [============================>.] - ETA: 0s - loss: 0.0015

Epoch 10: ReduceLROnPlateau reducing learning rate to 1e-06.
17408/17408 [==============================] - 56s 3ms/step - loss: 0.0015 - val_loss: 0.0017 - lr: 1.6000e-06

Epoch 11/15
17408/17408 [==============================] - 57s 3ms/step - loss: 0.0015 - val_loss: 0.0017 - lr: 1.0000e-06

Epoch 12/15
17408/17408 [==============================] - 55s 3ms/step - loss: 0.0015 - val_loss: 0.0017 - lr: 1.0000e-06

Epoch 13/15
17408/17408 [==============================] - 57s 3ms/step - loss: 0.0015 - val_loss: 0.0017 - lr: 1.0000e-06

Epoch 14/15
17408/17408 [==============================] - 57s 3ms/step - loss: 0.0015 - val_loss: 0.0017 - lr: 1.0000e-06

Epoch 15/15
17408/17408 [==============================] - 55s 3ms/step - loss: 0.0015 - val_loss: 0.0017 - lr: 1.0000e-06


Calculating activations statistics...
For each - except for last - layer the calculation is done twice: with and without the activation function
Training Completed...                                               
 
Total time of training the network: 872.881963833
