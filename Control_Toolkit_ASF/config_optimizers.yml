# TESTED:

mppi:
  seed: null # Seed for rng, for MPPI only, put null to set random seed (do it when you generate data for training!)
  mpc_horizon: 20                       # steps
  num_rollouts: 2000                    # Number of Monte Carlo samples
  LBD: 0.001                            # Cost parameter lambda
  NU: 1000.0                            # Exploration variance
  SQRTRHOINV: [ 0.5, 0.5 ]              # Sampling variance
  period_interpolation_inducing_points: 1                #interpolation stepsize when sampling, a random point is chosen every period_interpolation_inducing_points and horizon points in between are linearly interpolated

  # Double check that the two values below are the same as in the cost function config!!!
  cc_weight: 0.0
  R: 1.0                                # How much to punish Q

rpgd-tf:
  seed: null                            # If null, random seed based on datetime is used
  mpc_horizon: 15                       # steps
  SAMPLING_DISTRIBUTION: uniform  # "normal" or "uniform"
  period_interpolation_inducing_points: 2                #interpolation stepsize when sampling, a random point is chosen every period_interpolation_inducing_points and horizon points in between are linearly interpolated
  learning_rate: 0.05
  adam_beta_1: 0.9
  adam_beta_2: 0.999
  adam_epsilon: 1.0e-08
  gradmax_clip: 5
  rtol: 1.0e-3
  num_rollouts: 32
  opt_keep_k_ratio: 0.25
  outer_its: 2
  resamp_per: 5
#  sample_stdev: 0.2
#  sample_mean: 0.0
  sample_stdev: [ 0.3, 0.3 ] # With PID: [desired steering, desired speed]
  sample_mean: [0.0, 2.5] # With PID: [desired steering, desired speed]
  warmup: false
  warmup_iterations: 250




# NOT TESTED:

cem-tf:
  seed: null                          # If null, random seed based on datetime is used
  mpc_horizon: 40                       # steps
  cem_outer_it: 3                    #how many outer iterations to use
  cem_initial_action_stdev: 0.5
  num_rollouts: 200          #how many rollouts per outer cem iteration
  cem_stdev_min: 0.01
  cem_best_k: 40
  warmup: false
  warmup_iterations: 250
cem-gmm-tf:
  seed: null                          # If null, random seed based on datetime is used
  mpc_horizon: 40                       # steps
  cem_outer_it: 3                    #how many outer iterations to use
  num_rollouts: 200          #how many rollouts per outer cem iteration
  cem_stdev_min: 0.01
  cem_initial_action_stdev: 0.5
  cem_best_k: 40
cem-naive-grad-tf:
  seed: null                          # If null, random seed based on datetime is used
  mpc_horizon: 35                       # steps
  cem_outer_it: 1                       # how many outer iterations to use
  num_rollouts: 200                     # how many rollouts per outer cem iteration
  cem_stdev_min: 0.1
  cem_initial_action_stdev: 0.5
  cem_best_k: 40
  learning_rate: 0.1
  gradmax_clip: 10
cem-grad-bharadhwaj-tf:
  seed: null                          # If null, random seed based on datetime is used
  mpc_horizon: 50                       # steps
  learning_rate: 0.05
  adam_beta_1: 0.9
  adam_beta_2: 0.999
  adam_epsilon: 1.0e-08
  num_rollouts: 32
  cem_best_k: 8
  cem_outer_it: 2
  cem_initial_action_stdev: 2
  cem_stdev_min: 1.e-6
  gradmax_clip: 5
  warmup: false
  warmup_iterations: 250
gradient-tf:
  seed: null                            # If null, random seed based on datetime is used
  mpc_horizon: 35                       # steps
  learning_rate: 0.05
  adam_beta_1: 0.9
  adam_beta_2: 0.999
  adam_epsilon: 1.0e-07
  rtol: 1.0e-3
  gradient_steps: 5
  num_rollouts: 40
  initial_action_stdev: 0.5
  gradmax_clip: 5
  warmup: false
  warmup_iterations: 250
mppi-optimize-tf:
  seed: null                            # If null, random seed based on datetime is used
  mppi_LR: 0.02
  adam_beta_1: 0.4                      #default: 0.9
  adam_beta_2: 0.8                      #default: 0.999
  adam_epsilon: 1.0e-7                  #default: 1.0e-7
  gradmax_clip: 1000
  mpc_horizon: 35                       # steps
  num_rollouts: 400                     # Number of Monte Carlo samples
  cc_weight: 1.0
  R: 1.0                                # How much to punish Q, For MPPI YOU have to make sure that this is the same as in cost functions config, as it plays a special role in the optimization algorithm as well as is used in cost functions!
  LBD: 100.0                            # Cost parameter lambda
  NU: 1000.0                            # Exploration variance
  SQRTRHOINV: 0.02
  period_interpolation_inducing_points: 10                #interpolation stepsize when sampling, a random point is chosen every period_interpolation_inducing_points and horizon points in between are linearly interpolated
  optim_steps: 10
rpgd-me-tf:
  seed: null                            # If null, random seed based on datetime is used
  mpc_horizon: 40                       # steps
  SAMPLING_DISTRIBUTION: uniform  # "normal" or "uniform"
  period_interpolation_inducing_points: 1                #interpolation stepsize when sampling, a random point is chosen every period_interpolation_inducing_points and horizon points in between are linearly interpolated
  learning_rate: 0.01
  adam_beta_1: 0.9
  adam_beta_2: 0.999
  adam_epsilon: 1.0e-08
  maximum_entropy_alpha: 0.0
  gradmax_clip: 10
  rtol: 1.0e-3
  num_rollouts: 32
  opt_keep_k_ratio: 0.25
  outer_its: 25
  resamp_per: 1
  sample_stdev: 0.5
  warmup: false
  warmup_iterations: 250
rpgd-me-param-tf:
  seed: null                            # If null, random seed based on datetime is used
  mpc_horizon: 40                       # steps
  SAMPLING_DISTRIBUTION: uniform  # "normal" or "uniform"
  period_interpolation_inducing_points: 1                #interpolation stepsize when sampling, a random point is chosen every period_interpolation_inducing_points and horizon points in between are linearly interpolated
  learning_rate: 0.01
  adam_beta_1: 0.9
  adam_beta_2: 0.999
  adam_epsilon: 1.0e-08
  maximum_entropy_alpha: 0.0
  gradmax_clip: 10
  rtol: 1.0e-3
  num_rollouts: 32
  opt_keep_k_ratio: 0.25
  outer_its: 25
  resamp_per: 1
  sample_stdev: 0.5
  warmup: false
  warmup_iterations: 250
rpgd-ml-tf:
  seed: null                            # If null, random seed based on datetime is used
  mpc_horizon: 40                       # steps
  SAMPLING_DISTRIBUTION: uniform  # "normal" or "uniform"
  period_interpolation_inducing_points: 1                #interpolation stepsize when sampling, a random point is chosen every period_interpolation_inducing_points and horizon points in between are linearly interpolated
  learning_rate: 0.01
  adam_beta_1: 0.9
  adam_beta_2: 0.999
  adam_epsilon: 1.0e-08
  maximum_entropy_alpha: 0.1
  gradmax_clip: 10
  rtol: 1.0e-3
  num_rollouts: 32
  opt_keep_k_ratio: 0.25
  outer_its: 5
  resamp_per: 1
  sample_stdev: 0.5
  warmup: false
  warmup_iterations: 250
rpgd-particle-tf:
  seed: null                            # If null, random seed based on datetime is used
  mpc_horizon: 40                       # steps
  period_interpolation_inducing_points: 1                #interpolation stepsize when sampling, a random point is chosen every period_interpolation_inducing_points and horizon points in between are linearly interpolated
  learning_rate: 0.01
  adam_beta_1: 0.9
  adam_beta_2: 0.999
  adam_epsilon: 1.0e-08
  gradmax_clip: 10
  rtol: 1.0e-3
  num_rollouts: 32
  opt_keep_k_ratio: 0.25
  outer_its: 5
  resamp_per: 1
  sample_stdev: 0.5
  warmup: false
  warmup_iterations: 250
mppi-var-tf:
  seed: null                          # If null, random seed based on datetime is used
  mpc_horizon: 35                       # steps
  num_rollouts: 400                     # Number of Monte Carlo samples
  period_interpolation_inducing_points: 10                #interpolation stepsize when sampling, a random point is chosen every period_interpolation_inducing_points and horizon points in between are linearly interpolated
  cc_weight: 1.0
  R: 1.0                                # How much to punish Q
  # mc stands for mathematical correct, as this controller uses the formula from the paper
  LBD_mc: 10.0                          # Cost parameter lambda
  SQRTRHOINV_mc: 0.002                  # Sampling variance
  NU_mc: 20.0                           # Exploration variance
  LR: 1000                              # Learning rate for adaption of variance, !!! Set to 0 to retrieve a mppi version in accordance with mppi paper
  STDEV_min: 0.01                       # Maximal variance for sampling
  STDEV_max: 10                         # Minimal sampling variance for sampling
  max_grad_norm: 100000                 # max norm of gradient such that ||gradient||_2
random-action-tf:
  seed: null                          # Seed for rng, for MPPI only, put null to set random seed (do it when you generate data for training!)
  mpc_horizon: 35                      # steps
  num_rollouts: 320
nlp-forces:
  seed: null
  num_rollouts: 1
  mpc_horizon: 20                     # steps
  initial_guess: no_action                     # no_action or PD
  mppi_reinitialization: True
  generate_new_solver: False
#  generate_new_solver: True
  terminal_constraint_at_target: False
  terminal_set_width: 0.0               # Set to <=0 for no terminal set
  environment_specific_parameters:
    Car:
      dynamics: f1tenth_dynamics
#      cost: f1tenth_stage_cost
      cost: f1tenth_distance_to_waypoint
      target: f1tenth_target
      npar: 142
      dt: 0.03                            # Integration stepsize, make sure matches the one of the environment
#     optimize_over: [ 0, 1, 2, 3, 4, 5, 6, 7, 8]           #optimization variables idxs
      optimize_over: [5, 6, 8, 1, 2, 0, 7]
      is_angle: []                       # Which of the idxs refer to an angle
      action_max: [ 3.2, 9.5]
      state_max: [ inf, inf, 0.4189, 20.0, inf, inf, inf]        #
      idx_terminal_set: [ 2 ]             # Which state idx are affected by terminal set
      q: [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 ]    #State cost
      r: [ 0.0, 0.0 ]                                       #Input cost
#      self.lib.clip(nearest_waypoint_vel_x, 0.5, 17.5)
#      Args:
#        x (numpy.ndarray (3, )): vehicle state vector (x1, x2, x3, x4, x5)
#          x0: x position in global coordinates
#          x1: y position in global coordinates
#          x2: steering angle of front wheels           min: -0.4189  max = 0.4189
#          x3: velocity in longitudial direction
#          x4: yaw angle
#          x5: angular velocity
#          x6: slip_angle
#        u (numpy.ndarray (2, )): control input vector (u1, u2)
#          u0: steering angle velocity of front wheels   min: -3.2 max: 3.2
#          u1: longitudinal acceleration
#        Obs tensor( (9, ) )
#          s0: x5     yaw angle velocity
#          s1: x3     vel x
#          s2: x4     yaw angle
#          s3: /      cos(yaw)
#          s4: /      sin(yaw)
#          s5: x0     x
#          s6: x1     y
#          s7: x6     slip angle
#          s8: x2     steering angle
#
#
#
#
#      self.params = {
#          'mu': 1.0489,       # friction coefficient  [-]
#          'C_Sf': 4.718,      # cornering stiffness front [1/rad]
#          'C_Sr': 5.4562,     # cornering stiffness rear [1/rad]
#          'lf': 0.15875,      # distance from venter of gracity to front axle [m]
#          'lr': 0.17145,      # distance from venter of gracity to rear axle [m]
#          'h': 0.074,         # center of gravity height of toal mass [m]
#          'm': 3.74,          # Total Mass of car [kg]
#          'I': 0.04712,       # Moment of inertia for entire mass about z axis  [kgm^2]
#          's_min': -0.4189,   # Min steering angle [rad]
#          's_max': 0.4189,    # Max steering angle [rad]
#          'sv_min': -3.2,     # Min steering velocity [rad/s]
#          'sv_max': 3.2,      # Max steering velocity [rad/s]
#          'v_switch': 7.319,  # switching velocity [m/s]
#          'a_max': 9.51,      # Max acceleration [m/s^2]
#          'v_min':-5.0,       # Min velocity [m/s]
#          'v_max': 20.0,      # Max velocity [m/s]
#          'width': 0.31,      # Width of car [m]
#          'length': 0.58      # Length of car [m]
#          }