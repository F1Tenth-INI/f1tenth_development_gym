mpc:
  optimizer: mppi # 'mppi' or 'rpgd-tf'
  # optimizer: 'rpgd-tf' # 'mppi' or 'rpgd-tf'

  predictor_specification: ODE_TF
  cost_function_specification: 'racing'  # One of "default", "quadratic_boundary_grad", "quadratic_boundary_nonconvex", "quadratic_boundary"
  computation_library: tensorflow  # One of "numpy", "tensorflow", "pytorch". Defaults to "numpy" if none given.
  controller_logging: false
  calculate_optimal_trajectory: true

  online_learning:
    activated: False
    save_net_history_every_n_training_steps: 100
    controller_load_net_every_n_steps: 10
    buffer_length: 1  # Number of datapoints to train on
    train_every_n_steps: 1
    epochs_per_training: 1
    batch_size: 1
    optimizer: 'adam'  # SGD or Adam
    optimizers:
      adam:
        lr: 1.0e-4  # Default: 0.001
        # lr: 0.0
      SGD:
        lr: 0.001  # Default 0.01
        momentum: 0.0001  # Default 0.0
    data_filter: 'None'  # None, butterworth or averaging
    exponential_lr_decay:
      activated: false
      decay_rate: 0.95  # lr(t) = lr(0) * decay_rate^(t)
    reduce_lr_on_plateau:  # Does not work for multiple epochs
      activated: false
      factor: 0.1  # Updated as lr *= factor
      patience: 0  # How many steps to wait until lr is adjusted when higher loss is detected
      min_lr: 1.0e-6
      min_delta: 0.0

neural-imitator:
  seed: null                            # If null, random seed based on datetime is used
  PATH_TO_MODELS: './SI_Toolkit_ASF/Experiments/Obstacle_v1/Models/'
  net_name: 'Dense-89IN-64H1-64H2-2OUT-0'  # TF
  controller_logging: True
  computation_library: tensorflow
  input_at_input: true  # If true the network input is provided to step function, if not, system state is provided and controller tries to get the remaining inputs from environment parameters`
stanley:
  controller_logging: False
  computation_library: numpy